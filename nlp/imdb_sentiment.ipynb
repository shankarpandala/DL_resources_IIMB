{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"imdb_sentiment.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"5r8RDAtJYHfS","colab_type":"code","colab":{}},"source":["import pickle\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-7Cn_WPYHfU","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","\n","import utils;\n","from utils import *\n","from __future__ import division, print_function"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"acflzeWMYHfW","colab_type":"code","colab":{}},"source":["model_path = 'data/imdb/models/'\n","#%mkdir -p $model_path"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8NNgU8u5YHfX","colab_type":"text"},"source":["## Setup data"]},{"cell_type":"markdown","metadata":{"id":"3L9dEV8oYHfY","colab_type":"text"},"source":["We're going to look at the IMDB dataset, which contains movie reviews from IMDB, along with their sentiment. Keras comes with some helpers for this dataset."]},{"cell_type":"code","metadata":{"id":"pxfXvJ__YHfY","colab_type":"code","outputId":"bab1c660-372e-4880-cdae-1836954fb2d7","colab":{}},"source":["from keras.datasets import imdb\n","idx = imdb.get_word_index()   # dictionary mapping word to indices."],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"NrudI8JZYHfc","colab_type":"code","outputId":"30999951-eba9-4d16-f725-86bda77323a5","colab":{}},"source":["list(idx.items())[:5]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('3who', 88069),\n"," (\"'whore\", 78418),\n"," ('guy', 229),\n"," ('malik', 14870),\n"," ('elimination', 17132)]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"znOCIq8AYHfe","colab_type":"text"},"source":["This is the word list:"]},{"cell_type":"code","metadata":{"id":"EP04I7cAYHff","colab_type":"code","outputId":"4fc058a3-4cf3-4266-cc20-0beeec6acb8f","colab":{}},"source":["idx_arr = sorted(idx, key=idx.get)  # Sorting the dictionary keys based on ascending order of the dict values\n","idx_arr[:10]\n","\n","# Lower index indicates higher frequency."],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"fLY1aNH6YHfh","colab_type":"text"},"source":["...and this is the mapping from id to word"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"DqPnFOL-YHfh","colab_type":"code","colab":{}},"source":["#idx2word = {v: k for k, v in idx.iteritems()}\n","idx2word = {v: k for k, v in idx.items()}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZMhgIInNYHfj","colab_type":"text"},"source":["We download the reviews using code copied from keras.datasets:"]},{"cell_type":"code","metadata":{"id":"Kol5W_BgYHfk","colab_type":"code","outputId":"3cce5e6d-e8b7-458b-a2e5-4caee90cac2b","colab":{}},"source":["# get_file('imdb_full.pkl', origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl', md5_hash='d091312047c43cf9e4e38fef92437263')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/text-datasets/imdb_full.pkl\n","65495040/65552540 [============================>.] - ETA: 0s"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'C:\\\\Users\\\\bhansn\\\\.keras\\\\datasets\\\\imdb_full.pkl'"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"nDNO7vBHYHfn","colab_type":"code","colab":{}},"source":["path='C:\\\\Users\\\\bhansn\\\\.keras\\\\datasets\\\\imdb_full.pkl'\n","f = open(path, 'rb')\n","(x_train, labels_train), (x_test, labels_test) = pickle.load(f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"6nIUSEDCYHfp","colab_type":"code","outputId":"2da623ff-80ba-41db-d0a9-48158c8fa860","colab":{}},"source":["len(x_train)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25000"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"AK7YvOg0YHfr","colab_type":"code","outputId":"5227df74-94da-4956-ce20-090a6a06a354","colab":{}},"source":["x_train[0]  # List of words"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[23022,\n"," 309,\n"," 6,\n"," 3,\n"," 1069,\n"," 209,\n"," 9,\n"," 2175,\n"," 30,\n"," 1,\n"," 169,\n"," 55,\n"," 14,\n"," 46,\n"," 82,\n"," 5869,\n"," 41,\n"," 393,\n"," 110,\n"," 138,\n"," 14,\n"," 5359,\n"," 58,\n"," 4477,\n"," 150,\n"," 8,\n"," 1,\n"," 5032,\n"," 5948,\n"," 482,\n"," 69,\n"," 5,\n"," 261,\n"," 12,\n"," 23022,\n"," 73935,\n"," 2003,\n"," 6,\n"," 73,\n"," 2436,\n"," 5,\n"," 632,\n"," 71,\n"," 6,\n"," 5359,\n"," 1,\n"," 25279,\n"," 5,\n"," 2004,\n"," 10471,\n"," 1,\n"," 5941,\n"," 1534,\n"," 34,\n"," 67,\n"," 64,\n"," 205,\n"," 140,\n"," 65,\n"," 1232,\n"," 63526,\n"," 21145,\n"," 1,\n"," 49265,\n"," 4,\n"," 1,\n"," 223,\n"," 901,\n"," 29,\n"," 3024,\n"," 69,\n"," 4,\n"," 1,\n"," 5863,\n"," 10,\n"," 694,\n"," 2,\n"," 65,\n"," 1534,\n"," 51,\n"," 10,\n"," 216,\n"," 1,\n"," 387,\n"," 8,\n"," 60,\n"," 3,\n"," 1472,\n"," 3724,\n"," 802,\n"," 5,\n"," 3521,\n"," 177,\n"," 1,\n"," 393,\n"," 10,\n"," 1238,\n"," 14030,\n"," 30,\n"," 309,\n"," 3,\n"," 353,\n"," 344,\n"," 2989,\n"," 143,\n"," 130,\n"," 5,\n"," 7804,\n"," 28,\n"," 4,\n"," 126,\n"," 5359,\n"," 1472,\n"," 2375,\n"," 5,\n"," 23022,\n"," 309,\n"," 10,\n"," 532,\n"," 12,\n"," 108,\n"," 1470,\n"," 4,\n"," 58,\n"," 556,\n"," 101,\n"," 12,\n"," 23022,\n"," 309,\n"," 6,\n"," 227,\n"," 4187,\n"," 48,\n"," 3,\n"," 2237,\n"," 12,\n"," 9,\n"," 215]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"bpBcGfj0YHft","colab_type":"code","outputId":"881fe8ea-f3e3-40f1-f818-7cf587f84ebe","colab":{}},"source":["len(x_train[0])   # list of 138 word indices."],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["138"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"9xHvSEiMYHfv","colab_type":"text"},"source":["Here's the 1st review. As you see, the words have been replaced by ids. The ids can be looked up in idx2word."]},{"cell_type":"code","metadata":{"id":"EBatFdUuYHfw","colab_type":"code","outputId":"b5c0d04e-8af1-407a-968f-de94ec64314f","colab":{}},"source":["', '.join(map(str, x_train[0]))  # Converting from list of indices to one string of comma-sep indices."],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'23022, 309, 6, 3, 1069, 209, 9, 2175, 30, 1, 169, 55, 14, 46, 82, 5869, 41, 393, 110, 138, 14, 5359, 58, 4477, 150, 8, 1, 5032, 5948, 482, 69, 5, 261, 12, 23022, 73935, 2003, 6, 73, 2436, 5, 632, 71, 6, 5359, 1, 25279, 5, 2004, 10471, 1, 5941, 1534, 34, 67, 64, 205, 140, 65, 1232, 63526, 21145, 1, 49265, 4, 1, 223, 901, 29, 3024, 69, 4, 1, 5863, 10, 694, 2, 65, 1534, 51, 10, 216, 1, 387, 8, 60, 3, 1472, 3724, 802, 5, 3521, 177, 1, 393, 10, 1238, 14030, 30, 309, 3, 353, 344, 2989, 143, 130, 5, 7804, 28, 4, 126, 5359, 1472, 2375, 5, 23022, 309, 10, 532, 12, 108, 1470, 4, 58, 556, 101, 12, 23022, 309, 6, 227, 4187, 48, 3, 2237, 12, 9, 215'"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"Cxlgbe6_YHfy","colab_type":"text"},"source":["The first word of the first review is 23022. Let's see what that is."]},{"cell_type":"code","metadata":{"id":"B2cYQDSOYHfz","colab_type":"code","outputId":"664500cd-0776-4d5d-b286-c597670a1bfa","colab":{}},"source":["idx2word[23022]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'bromwell'"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"yAdOpjpqYHf1","colab_type":"text"},"source":["Here's the whole review, mapped from ids to words."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Qgwlbm1vYHf2","colab_type":"code","outputId":"93c187ba-5c33-41fc-b088-6b4e381b5d9e","colab":{}},"source":["' '.join([idx2word[o] for o in x_train[0]])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell high's satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers' pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i'm here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isn't\""]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"AUyDURyKYHf4","colab_type":"text"},"source":["The labels are 1 for positive, 0 for negative."]},{"cell_type":"code","metadata":{"id":"p_LYsSlQYHf5","colab_type":"code","outputId":"513b89f4-44b8-43fd-e3ff-929db43c8f21","colab":{}},"source":["labels_train[:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"FduGjBCJYHf8","colab_type":"text"},"source":["Reduce vocab size by setting rare words to max index."]},{"cell_type":"code","metadata":{"id":"Z4mwfKbMYHf8","colab_type":"code","colab":{}},"source":["vocab_size = 5000"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h4XGYPMYYHf-","colab_type":"text"},"source":["Since the lower index indicates higher frquency, we can set all the index values beyond 5000, which are rare words, to 5000 "]},{"cell_type":"code","metadata":{"id":"xaq9IX4EYHf_","colab_type":"code","colab":{}},"source":["trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_train]\n","test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_test]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"79ezhy9RYHgA","colab_type":"text"},"source":["Look at distribution of lengths of sentences."]},{"cell_type":"code","metadata":{"id":"DXAGoCL6YHgB","colab_type":"code","outputId":"e3a6eaf4-bc44-4563-aa5a-f2206c0ebc60","colab":{}},"source":["lens = list(map(len, trn))\n","lens"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[138,\n"," 433,\n"," 149,\n"," 124,\n"," 121,\n"," 181,\n"," 111,\n"," 343,\n"," 448,\n"," 327,\n"," 287,\n"," 87,\n"," 281,\n"," 228,\n"," 145,\n"," 159,\n"," 198,\n"," 324,\n"," 131,\n"," 154,\n"," 216,\n"," 573,\n"," 176,\n"," 186,\n"," 145,\n"," 108,\n"," 109,\n"," 312,\n"," 152,\n"," 142,\n"," 248,\n"," 121,\n"," 163,\n"," 171,\n"," 444,\n"," 480,\n"," 57,\n"," 300,\n"," 223,\n"," 338,\n"," 360,\n"," 163,\n"," 536,\n"," 242,\n"," 112,\n"," 332,\n"," 986,\n"," 558,\n"," 88,\n"," 1853,\n"," 447,\n"," 541,\n"," 184,\n"," 545,\n"," 206,\n"," 730,\n"," 180,\n"," 163,\n"," 55,\n"," 277,\n"," 98,\n"," 182,\n"," 133,\n"," 224,\n"," 323,\n"," 166,\n"," 106,\n"," 225,\n"," 110,\n"," 96,\n"," 125,\n"," 195,\n"," 295,\n"," 327,\n"," 43,\n"," 149,\n"," 109,\n"," 331,\n"," 73,\n"," 232,\n"," 119,\n"," 261,\n"," 119,\n"," 117,\n"," 55,\n"," 56,\n"," 271,\n"," 129,\n"," 176,\n"," 164,\n"," 254,\n"," 77,\n"," 203,\n"," 120,\n"," 174,\n"," 55,\n"," 64,\n"," 103,\n"," 138,\n"," 168,\n"," 174,\n"," 218,\n"," 114,\n"," 164,\n"," 594,\n"," 165,\n"," 278,\n"," 253,\n"," 147,\n"," 513,\n"," 242,\n"," 348,\n"," 196,\n"," 56,\n"," 166,\n"," 121,\n"," 275,\n"," 382,\n"," 285,\n"," 360,\n"," 188,\n"," 108,\n"," 230,\n"," 192,\n"," 141,\n"," 222,\n"," 520,\n"," 160,\n"," 115,\n"," 111,\n"," 158,\n"," 145,\n"," 222,\n"," 149,\n"," 329,\n"," 175,\n"," 146,\n"," 212,\n"," 654,\n"," 242,\n"," 319,\n"," 202,\n"," 344,\n"," 134,\n"," 318,\n"," 105,\n"," 133,\n"," 167,\n"," 122,\n"," 635,\n"," 151,\n"," 174,\n"," 417,\n"," 299,\n"," 503,\n"," 339,\n"," 399,\n"," 157,\n"," 153,\n"," 185,\n"," 189,\n"," 133,\n"," 135,\n"," 297,\n"," 509,\n"," 89,\n"," 402,\n"," 337,\n"," 154,\n"," 156,\n"," 96,\n"," 162,\n"," 149,\n"," 183,\n"," 75,\n"," 114,\n"," 130,\n"," 135,\n"," 118,\n"," 403,\n"," 147,\n"," 129,\n"," 143,\n"," 148,\n"," 162,\n"," 222,\n"," 129,\n"," 153,\n"," 134,\n"," 150,\n"," 201,\n"," 136,\n"," 465,\n"," 279,\n"," 162,\n"," 428,\n"," 408,\n"," 162,\n"," 160,\n"," 160,\n"," 273,\n"," 183,\n"," 136,\n"," 137,\n"," 646,\n"," 500,\n"," 139,\n"," 123,\n"," 539,\n"," 425,\n"," 55,\n"," 514,\n"," 153,\n"," 162,\n"," 614,\n"," 99,\n"," 255,\n"," 97,\n"," 262,\n"," 168,\n"," 390,\n"," 981,\n"," 124,\n"," 132,\n"," 142,\n"," 156,\n"," 149,\n"," 166,\n"," 99,\n"," 194,\n"," 44,\n"," 377,\n"," 194,\n"," 652,\n"," 204,\n"," 553,\n"," 80,\n"," 330,\n"," 215,\n"," 435,\n"," 89,\n"," 127,\n"," 360,\n"," 472,\n"," 158,\n"," 310,\n"," 180,\n"," 363,\n"," 214,\n"," 242,\n"," 82,\n"," 109,\n"," 412,\n"," 480,\n"," 324,\n"," 154,\n"," 214,\n"," 745,\n"," 119,\n"," 210,\n"," 136,\n"," 199,\n"," 81,\n"," 396,\n"," 135,\n"," 122,\n"," 236,\n"," 199,\n"," 364,\n"," 429,\n"," 166,\n"," 170,\n"," 116,\n"," 329,\n"," 250,\n"," 271,\n"," 318,\n"," 746,\n"," 139,\n"," 79,\n"," 221,\n"," 136,\n"," 61,\n"," 141,\n"," 214,\n"," 92,\n"," 457,\n"," 390,\n"," 316,\n"," 156,\n"," 161,\n"," 40,\n"," 279,\n"," 65,\n"," 68,\n"," 138,\n"," 132,\n"," 155,\n"," 72,\n"," 545,\n"," 39,\n"," 80,\n"," 127,\n"," 383,\n"," 119,\n"," 249,\n"," 142,\n"," 108,\n"," 221,\n"," 162,\n"," 297,\n"," 497,\n"," 639,\n"," 264,\n"," 335,\n"," 449,\n"," 569,\n"," 257,\n"," 97,\n"," 36,\n"," 58,\n"," 340,\n"," 344,\n"," 200,\n"," 119,\n"," 225,\n"," 153,\n"," 62,\n"," 75,\n"," 114,\n"," 183,\n"," 157,\n"," 167,\n"," 300,\n"," 55,\n"," 86,\n"," 133,\n"," 149,\n"," 239,\n"," 67,\n"," 167,\n"," 403,\n"," 133,\n"," 291,\n"," 308,\n"," 458,\n"," 1290,\n"," 44,\n"," 189,\n"," 120,\n"," 611,\n"," 202,\n"," 204,\n"," 1032,\n"," 364,\n"," 474,\n"," 268,\n"," 123,\n"," 373,\n"," 234,\n"," 207,\n"," 170,\n"," 109,\n"," 685,\n"," 190,\n"," 117,\n"," 169,\n"," 196,\n"," 500,\n"," 196,\n"," 359,\n"," 246,\n"," 253,\n"," 172,\n"," 236,\n"," 265,\n"," 925,\n"," 347,\n"," 173,\n"," 184,\n"," 143,\n"," 186,\n"," 255,\n"," 173,\n"," 195,\n"," 266,\n"," 30,\n"," 235,\n"," 322,\n"," 138,\n"," 220,\n"," 148,\n"," 356,\n"," 578,\n"," 185,\n"," 869,\n"," 703,\n"," 196,\n"," 162,\n"," 149,\n"," 346,\n"," 213,\n"," 351,\n"," 352,\n"," 343,\n"," 191,\n"," 155,\n"," 206,\n"," 144,\n"," 135,\n"," 62,\n"," 194,\n"," 121,\n"," 185,\n"," 174,\n"," 219,\n"," 165,\n"," 129,\n"," 118,\n"," 125,\n"," 185,\n"," 793,\n"," 142,\n"," 145,\n"," 157,\n"," 166,\n"," 195,\n"," 138,\n"," 160,\n"," 297,\n"," 252,\n"," 138,\n"," 183,\n"," 249,\n"," 129,\n"," 47,\n"," 64,\n"," 162,\n"," 238,\n"," 511,\n"," 463,\n"," 185,\n"," 309,\n"," 431,\n"," 102,\n"," 105,\n"," 247,\n"," 138,\n"," 300,\n"," 77,\n"," 293,\n"," 492,\n"," 897,\n"," 101,\n"," 728,\n"," 134,\n"," 164,\n"," 411,\n"," 270,\n"," 84,\n"," 282,\n"," 227,\n"," 178,\n"," 239,\n"," 406,\n"," 143,\n"," 675,\n"," 143,\n"," 206,\n"," 567,\n"," 134,\n"," 447,\n"," 258,\n"," 270,\n"," 152,\n"," 135,\n"," 112,\n"," 322,\n"," 617,\n"," 158,\n"," 193,\n"," 183,\n"," 113,\n"," 249,\n"," 374,\n"," 267,\n"," 549,\n"," 406,\n"," 239,\n"," 235,\n"," 764,\n"," 252,\n"," 88,\n"," 255,\n"," 275,\n"," 181,\n"," 236,\n"," 71,\n"," 143,\n"," 514,\n"," 144,\n"," 170,\n"," 254,\n"," 189,\n"," 625,\n"," 255,\n"," 213,\n"," 102,\n"," 95,\n"," 87,\n"," 453,\n"," 118,\n"," 119,\n"," 36,\n"," 718,\n"," 306,\n"," 137,\n"," 211,\n"," 167,\n"," 112,\n"," 266,\n"," 121,\n"," 471,\n"," 207,\n"," 145,\n"," 82,\n"," 221,\n"," 155,\n"," 197,\n"," 174,\n"," 220,\n"," 125,\n"," 266,\n"," 49,\n"," 211,\n"," 113,\n"," 211,\n"," 431,\n"," 146,\n"," 167,\n"," 158,\n"," 147,\n"," 107,\n"," 198,\n"," 130,\n"," 90,\n"," 118,\n"," 72,\n"," 124,\n"," 179,\n"," 106,\n"," 120,\n"," 108,\n"," 51,\n"," 118,\n"," 157,\n"," 130,\n"," 157,\n"," 148,\n"," 387,\n"," 132,\n"," 86,\n"," 160,\n"," 126,\n"," 253,\n"," 245,\n"," 127,\n"," 151,\n"," 277,\n"," 68,\n"," 112,\n"," 183,\n"," 71,\n"," 824,\n"," 122,\n"," 289,\n"," 149,\n"," 181,\n"," 295,\n"," 412,\n"," 196,\n"," 193,\n"," 103,\n"," 280,\n"," 899,\n"," 139,\n"," 332,\n"," 109,\n"," 341,\n"," 278,\n"," 133,\n"," 177,\n"," 126,\n"," 110,\n"," 122,\n"," 164,\n"," 148,\n"," 126,\n"," 124,\n"," 196,\n"," 135,\n"," 113,\n"," 399,\n"," 220,\n"," 301,\n"," 514,\n"," 184,\n"," 144,\n"," 792,\n"," 118,\n"," 151,\n"," 283,\n"," 284,\n"," 648,\n"," 119,\n"," 253,\n"," 1215,\n"," 85,\n"," 128,\n"," 106,\n"," 203,\n"," 64,\n"," 404,\n"," 503,\n"," 45,\n"," 328,\n"," 51,\n"," 283,\n"," 129,\n"," 89,\n"," 112,\n"," 150,\n"," 120,\n"," 805,\n"," 402,\n"," 310,\n"," 218,\n"," 74,\n"," 132,\n"," 554,\n"," 124,\n"," 235,\n"," 458,\n"," 60,\n"," 136,\n"," 534,\n"," 175,\n"," 445,\n"," 334,\n"," 296,\n"," 485,\n"," 77,\n"," 370,\n"," 109,\n"," 46,\n"," 124,\n"," 237,\n"," 180,\n"," 113,\n"," 513,\n"," 982,\n"," 129,\n"," 176,\n"," 45,\n"," 255,\n"," 76,\n"," 377,\n"," 175,\n"," 157,\n"," 124,\n"," 353,\n"," 35,\n"," 42,\n"," 143,\n"," 95,\n"," 57,\n"," 157,\n"," 125,\n"," 137,\n"," 79,\n"," 187,\n"," 330,\n"," 183,\n"," 83,\n"," 430,\n"," 222,\n"," 751,\n"," 232,\n"," 94,\n"," 185,\n"," 64,\n"," 95,\n"," 209,\n"," 179,\n"," 264,\n"," 183,\n"," 465,\n"," 95,\n"," 140,\n"," 59,\n"," 130,\n"," 148,\n"," 264,\n"," 150,\n"," 212,\n"," 106,\n"," 128,\n"," 69,\n"," 91,\n"," 641,\n"," 49,\n"," 493,\n"," 143,\n"," 223,\n"," 491,\n"," 193,\n"," 318,\n"," 223,\n"," 172,\n"," 135,\n"," 107,\n"," 64,\n"," 70,\n"," 146,\n"," 132,\n"," 192,\n"," 190,\n"," 156,\n"," 174,\n"," 136,\n"," 127,\n"," 135,\n"," 148,\n"," 125,\n"," 391,\n"," 346,\n"," 508,\n"," 145,\n"," 234,\n"," 661,\n"," 288,\n"," 296,\n"," 22,\n"," 59,\n"," 49,\n"," 54,\n"," 280,\n"," 80,\n"," 415,\n"," 118,\n"," 294,\n"," 486,\n"," 119,\n"," 270,\n"," 562,\n"," 167,\n"," 238,\n"," 568,\n"," 308,\n"," 109,\n"," 156,\n"," 119,\n"," 123,\n"," 138,\n"," 52,\n"," 179,\n"," 46,\n"," 93,\n"," 127,\n"," 378,\n"," 361,\n"," 47,\n"," 131,\n"," 115,\n"," 294,\n"," 379,\n"," 161,\n"," 185,\n"," 100,\n"," 132,\n"," 179,\n"," 131,\n"," 324,\n"," 116,\n"," 128,\n"," 160,\n"," 182,\n"," 448,\n"," 56,\n"," 221,\n"," 459,\n"," 121,\n"," 179,\n"," 549,\n"," 135,\n"," 313,\n"," 204,\n"," 243,\n"," 357,\n"," 178,\n"," 113,\n"," 137,\n"," 136,\n"," 222,\n"," 167,\n"," 115,\n"," 168,\n"," 216,\n"," 139,\n"," 178,\n"," 133,\n"," 220,\n"," 103,\n"," 146,\n"," 111,\n"," 200,\n"," 62,\n"," 55,\n"," 82,\n"," 299,\n"," 154,\n"," 263,\n"," 140,\n"," 154,\n"," 146,\n"," 390,\n"," 114,\n"," 215,\n"," 67,\n"," 240,\n"," 155,\n"," 135,\n"," 56,\n"," 477,\n"," 1014,\n"," 247,\n"," 337,\n"," 143,\n"," 127,\n"," 55,\n"," 379,\n"," 608,\n"," 538,\n"," 68,\n"," 57,\n"," 747,\n"," 483,\n"," 132,\n"," 58,\n"," 101,\n"," 116,\n"," 139,\n"," 235,\n"," 129,\n"," 488,\n"," 49,\n"," 150,\n"," 44,\n"," 160,\n"," 55,\n"," 66,\n"," 209,\n"," 230,\n"," 160,\n"," 123,\n"," 147,\n"," 122,\n"," 113,\n"," 280,\n"," 155,\n"," 70,\n"," 129,\n"," 135,\n"," 140,\n"," 154,\n"," 127,\n"," 589,\n"," 121,\n"," 142,\n"," 54,\n"," 270,\n"," 60,\n"," 127,\n"," 119,\n"," 274,\n"," 181,\n"," 294,\n"," 106,\n"," 119,\n"," 111,\n"," 178,\n"," 116,\n"," 140,\n"," 195,\n"," 47,\n"," 70,\n"," 171,\n"," 50,\n"," 83,\n"," 226,\n"," 77,\n"," 132,\n"," 621,\n"," 191,\n"," 240,\n"," 95,\n"," 169,\n"," 85,\n"," 367,\n"," 166,\n"," 64,\n"," 491,\n"," 134,\n"," 213,\n"," 306,\n"," 132,\n"," 100,\n"," 122,\n"," 133,\n"," 36,\n"," 58,\n"," 152,\n"," 88,\n"," 110,\n"," 74,\n"," 171,\n"," 128,\n"," 139,\n"," 118,\n"," 108,\n"," 142,\n"," 239,\n"," 93,\n"," 385,\n"," 37,\n"," 166,\n"," 391,\n"," 75,\n"," 175,\n"," 381,\n"," 125,\n"," 287,\n"," 121,\n"," 191,\n"," 147,\n"," 253,\n"," 382,\n"," 328,\n"," 389,\n"," 158,\n"," 188,\n"," 154,\n"," 124,\n"," 173,\n"," 91,\n"," 135,\n"," 695,\n"," 256,\n"," 621,\n"," 404,\n"," 138,\n"," 170,\n"," 153,\n"," 132,\n"," 147,\n"," 131,\n"," 146,\n"," 174,\n"," 364,\n"," 192,\n"," 266,\n"," 309,\n"," 177,\n"," 156,\n"," 164,\n"," 137,\n"," 230,\n"," 114,\n"," 243,\n"," 103,\n"," 338,\n"," 385,\n"," 429,\n"," 489,\n"," 179,\n"," 217,\n"," 150,\n"," 134,\n"," 127,\n"," 159,\n"," 144,\n"," 141,\n"," 108,\n"," 129,\n"," 192,\n"," 345,\n"," 151,\n"," 150,\n"," 126,\n"," 78,\n"," ...]"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"CnqDj-tpYHgE","colab_type":"code","outputId":"b8870459-5f24-456a-b7f0-523dd5b7cc4f","colab":{}},"source":["#(lens.max(), lens.min(), lens.mean())\n","max(lens), min(lens), np.mean(lens)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2493, 10, 237.71364)"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"2XLE2UJMYHgH","colab_type":"text"},"source":["Pad (with zero) or truncate each sentence to make consistent length."]},{"cell_type":"code","metadata":{"id":"697sjL-BYHgH","colab_type":"code","colab":{}},"source":["seq_len = 500\n","\n","trn = sequence.pad_sequences(trn, maxlen=seq_len, value=0)\n","test = sequence.pad_sequences(test, maxlen=seq_len, value=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rFK9kOztYHgJ","colab_type":"text"},"source":["This results in nice rectangular matrices that can be passed to ML algorithms. Reviews shorter than 500 words are pre-padded with zeros, those greater are truncated."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"mqLWYkSFYHgJ","colab_type":"code","outputId":"c84863d8-5a43-4f88-bc3e-eefc7982f348","colab":{}},"source":["trn.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25000, 500)"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"DRYN2p_qYHgL","colab_type":"text"},"source":["## Create simple models"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"hPCfTr1QYHgM","colab_type":"text"},"source":["### Single hidden layer NN"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"YdwI2gn3YHgM","colab_type":"text"},"source":["The simplest model that tends to give reasonable results is a single hidden layer net. So let's try that. Note that we can't expect to get any useful results by feeding word ids directly into a neural net - so instead we use an embedding to replace them with a vector of 32 (initially random) floats for each word in the vocab."]},{"cell_type":"code","metadata":{"hidden":true,"id":"fcZASWEBYHgM","colab_type":"code","colab":{}},"source":["model = Sequential([\n","    Embedding(vocab_size, 32, input_length=seq_len),\n","    Flatten(),\n","    Dense(100, activation='relu'),\n","    Dropout(0.7),\n","    Dense(1, activation='sigmoid')])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"scrolled":true,"id":"mr84vr-TYHgO","colab_type":"code","outputId":"cf3db028-a9ee-4197-9f6d-dde54631f682","colab":{}},"source":["model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["____________________________________________________________________________________________________\n","Layer (type)                     Output Shape          Param #     Connected to                     \n","====================================================================================================\n","embedding_1 (Embedding)          (None, 500, 32)       160000      embedding_input_1[0][0]          \n","____________________________________________________________________________________________________\n","flatten_1 (Flatten)              (None, 16000)         0           embedding_1[0][0]                \n","____________________________________________________________________________________________________\n","dense_1 (Dense)                  (None, 100)           1600100     flatten_1[0][0]                  \n","____________________________________________________________________________________________________\n","dropout_1 (Dropout)              (None, 100)           0           dense_1[0][0]                    \n","____________________________________________________________________________________________________\n","dense_2 (Dense)                  (None, 1)             101         dropout_1[0][0]                  \n","====================================================================================================\n","Total params: 1760201\n","____________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"hidden":true,"id":"H0yJiXiSYHgP","colab_type":"code","outputId":"7454ba49-9d29-40c4-a753-9eb51e45e96a","colab":{}},"source":["model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 25000 samples, validate on 25000 samples\n","Epoch 1/2\n","25000/25000 [==============================] - 1s - loss: 0.4651 - acc: 0.7495 - val_loss: 0.2830 - val_acc: 0.8804\n","Epoch 2/2\n","25000/25000 [==============================] - 1s - loss: 0.1969 - acc: 0.9265 - val_loss: 0.3195 - val_acc: 0.8694\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f0e084f4210>"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"6_z5YapLYHgR","colab_type":"text"},"source":["### Single conv layer with max pooling"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"vF64qnQ-YHgS","colab_type":"text"},"source":["A CNN is likely to work better, since it's designed to take advantage of ordered data. We'll need to use a 1D CNN, since a sequence of words is 1D."]},{"cell_type":"code","metadata":{"hidden":true,"id":"L8eJuyzWYHgS","colab_type":"code","colab":{}},"source":["conv1 = Sequential([\n","    Embedding(vocab_size, 32, input_length=seq_len, dropout=0.2),\n","    Dropout(0.2),\n","    Convolution1D(64, 5, border_mode='same', activation='relu'),\n","    Dropout(0.2),\n","    MaxPooling1D(),\n","    Flatten(),\n","    Dense(100, activation='relu'),\n","    Dropout(0.7),\n","    Dense(1, activation='sigmoid')])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"7QlgMeueYHgT","colab_type":"code","colab":{}},"source":["conv1.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"scrolled":false,"id":"NKh_gyQ9YHgU","colab_type":"code","outputId":"769ed4ae-87c3-470e-b86b-0b2601ca7bd0","colab":{}},"source":["conv1.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=4, batch_size=64)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 25000 samples, validate on 25000 samples\n","Epoch 1/4\n","25000/25000 [==============================] - 4s - loss: 0.4984 - acc: 0.7250 - val_loss: 0.2922 - val_acc: 0.8816\n","Epoch 2/4\n","25000/25000 [==============================] - 4s - loss: 0.2971 - acc: 0.8836 - val_loss: 0.2681 - val_acc: 0.8911\n","Epoch 3/4\n","25000/25000 [==============================] - 4s - loss: 0.2568 - acc: 0.8983 - val_loss: 0.2551 - val_acc: 0.8947\n","Epoch 4/4\n","25000/25000 [==============================] - 4s - loss: 0.2427 - acc: 0.9029 - val_loss: 0.2558 - val_acc: 0.8947\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f99cfa785d0>"]},"metadata":{"tags":[]},"execution_count":278}]},{"cell_type":"code","metadata":{"hidden":true,"id":"4XcfrKMaYHgX","colab_type":"code","colab":{}},"source":["conv1.save_weights(model_path + 'conv1.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"f2zeTQfMYHgY","colab_type":"code","colab":{}},"source":["conv1.load_weights(model_path + 'conv1.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"2W2f1P34YHga","colab_type":"text"},"source":["## Pre-trained vectors"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"ZUdZ8_fGYHgb","colab_type":"text"},"source":["You may want to look at wordvectors.ipynb before moving on.\n","\n","In this section, we replicate the previous CNN, but using pre-trained embeddings."]},{"cell_type":"code","metadata":{"hidden":true,"id":"yHDj_QBvYHgb","colab_type":"code","colab":{}},"source":["def get_glove_dataset(dataset):\n","    \"\"\"Download the requested glove dataset from files.fast.ai\n","    and return a location that can be passed to load_vectors.\n","    \"\"\"\n","    # see wordvectors.ipynb for info on how these files were\n","    # generated from the original glove data.\n","    md5sums = {'6B.50d': '8e1557d1228decbda7db6dfd81cd9909',\n","               '6B.100d': 'c92dbbeacde2b0384a43014885a60b2c',\n","               '6B.200d': 'af271b46c04b0b2e41a84d8cd806178d',\n","               '6B.300d': '30290210376887dcc6d0a5a6374d8255'}\n","    glove_path = os.path.abspath('data/glove/results')\n","    %mkdir -p $glove_path\n","    return get_file(dataset,\n","                    'http://files.fast.ai/models/glove/' + dataset + '.tgz',\n","                    cache_subdir=glove_path,\n","                    md5_hash=md5sums.get(dataset, None),\n","                    untar=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"NdHje19zYHgc","colab_type":"code","colab":{}},"source":["def load_vectors(loc):\n","    return (load_array(loc+'.dat'), \n","            pickle.load(open(loc+'_words.pkl','rb'),encoding='Latin1'), \n","            pickle.load(open(loc+'_idx.pkl','rb'), encoding='Latin1'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"GiGrgtxbYHgd","colab_type":"code","colab":{}},"source":["#vecs, words, wordidx = load_vectors(get_glove_dataset('6B.50d'))\n","\n","vecs, words, wordidx = load_vectors('data\\\\glove\\\\results\\\\6B.50d')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"-oxgplhBYHgf","colab_type":"text"},"source":["The glove word ids and imdb word ids use different indexes. So we create a simple function that creates an embedding matrix using the indexes from imdb, and the embeddings from glove (where they exist)."]},{"cell_type":"code","metadata":{"hidden":true,"id":"DeKjcc3YYHgh","colab_type":"code","colab":{}},"source":["def create_emb():\n","    n_fact = vecs.shape[1]\n","    emb = np.zeros((vocab_size, n_fact))\n","\n","    for i in range(1,len(emb)):\n","        word = idx2word[i]\n","        if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n","            src_idx = wordidx[word]\n","            emb[i] = vecs[src_idx]\n","        else:\n","            # If we can't find the word in glove, randomly initialize\n","            emb[i] = normal(scale=0.6, size=(n_fact,))\n","\n","    # This is our \"rare word\" id - we want to randomly initialize\n","    emb[-1] = normal(scale=0.6, size=(n_fact,))\n","    emb/=3\n","    return emb"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"xEdjUh5_YHgm","colab_type":"code","colab":{}},"source":["emb = create_emb()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"zeUbQV1xYHgq","colab_type":"text"},"source":["We pass our embedding matrix to the Embedding constructor, and set it to non-trainable."]},{"cell_type":"code","metadata":{"hidden":true,"id":"jCsPp2eJYHgr","colab_type":"code","outputId":"0c2a07c4-6df1-4396-f5c2-9f421bb5949c","colab":{}},"source":["model = Sequential([\n","    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, \n","              weights=[emb], trainable=False),\n","    Dropout(0.25),\n","    Convolution1D(64, 5, border_mode='same', activation='relu'),\n","    Dropout(0.25),\n","    MaxPooling1D(),\n","    Flatten(),\n","    Dense(100, activation='relu'),\n","    Dropout(0.7),\n","    Dense(1, activation='sigmoid')])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["C:\\tools\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","C:\\tools\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 5, activation=\"relu\", padding=\"same\")`\n","  \"\"\"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"hidden":true,"id":"PIwZ56PaYHgw","colab_type":"code","outputId":"e91c015a-b59c-4e2c-b404-bfe34c6e8595","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 500, 50)           250000    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 500, 50)           0         \n","_________________________________________________________________\n","conv1d_1 (Conv1D)            (None, 500, 64)           16064     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 500, 64)           0         \n","_________________________________________________________________\n","max_pooling1d_1 (MaxPooling1 (None, 250, 64)           0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 16000)             0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 100)               1600100   \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 100)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 101       \n","=================================================================\n","Total params: 1,866,265\n","Trainable params: 1,616,265\n","Non-trainable params: 250,000\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"hidden":true,"id":"tsD05NOPYHg1","colab_type":"code","colab":{}},"source":["model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"scrolled":true,"id":"WUSApEF9YHg4","colab_type":"code","outputId":"4676291f-d4c8-4b9f-cf4e-c99e09ed0e18","colab":{}},"source":["model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 25000 samples, validate on 25000 samples\n","Epoch 1/2\n","25000/25000 [==============================] - 4s - loss: 0.5217 - acc: 0.7172 - val_loss: 0.2942 - val_acc: 0.8815\n","Epoch 2/2\n","25000/25000 [==============================] - 4s - loss: 0.3169 - acc: 0.8719 - val_loss: 0.2662 - val_acc: 0.8978\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f0de0f2d910>"]},"metadata":{"tags":[]},"execution_count":90}]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"8J9IhraAYHg8","colab_type":"text"},"source":["We already have beaten our previous model! But let's fine-tune the embedding weights - especially since the words we couldn't find in glove just have random embeddings."]},{"cell_type":"code","metadata":{"hidden":true,"id":"ukuYzSpGYHg8","colab_type":"code","colab":{}},"source":["model.layers[0].trainable=True   # Embedding Layer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"id":"IdZK4tF2YHg-","colab_type":"code","colab":{}},"source":["model.optimizer.lr=1e-4"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"hidden":true,"scrolled":true,"id":"aFz955U_YHg_","colab_type":"code","outputId":"70a50be0-e2da-4fde-c395-13237c601ad1","colab":{}},"source":["model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=1, batch_size=64)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 25000 samples, validate on 25000 samples\n","Epoch 1/1\n","25000/25000 [==============================] - 4s - loss: 0.2751 - acc: 0.8911 - val_loss: 0.2500 - val_acc: 0.9008\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f0de0c4e0d0>"]},"metadata":{"tags":[]},"execution_count":93}]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"jNG-XnsDYHho","colab_type":"text"},"source":["As expected, that's given us a nice little boost. :)"]}]}